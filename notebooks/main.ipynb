{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTSKYSA8zaiE",
        "outputId": "b1eee553-90c7-4879-84c5-276ca277ee8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=6b0cdae36b2281897a09ecdcb7bd234061bab7d2333306f0bb9050b7dd540014\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install fpdf\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from fpdf import FPDF\n",
        "import nbformat as nbf\n",
        "\n",
        "# Criando diretórios do projeto\n",
        "base_dir = 'churn_project'\n",
        "dirs = ['data', 'models', 'reports', 'scripts', 'notebooks', 'visualizations']\n",
        "for dir_name in dirs:\n",
        "    os.makedirs(os.path.join(base_dir, dir_name), exist_ok=True)\n",
        "\n",
        "# Simulação e preparação dos dados\n",
        "def simulate_data():\n",
        "    \"\"\"\n",
        "    Simula dados de clientes para análise de churn.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    num_samples = 1000\n",
        "    data = pd.DataFrame({\n",
        "        'CustomerID': np.arange(1, num_samples + 1),\n",
        "        'Gender': np.random.choice(['Male', 'Female'], num_samples),\n",
        "        'Age': np.random.randint(18, 80, num_samples),\n",
        "        'Tenure': np.random.randint(0, 72, num_samples),\n",
        "        'ServiceType': np.random.choice(['Basic', 'Premium', 'Ultimate'], num_samples),\n",
        "        'MonthlyCharges': np.random.uniform(20, 150, num_samples),\n",
        "        'Complaints': np.random.randint(0, 10, num_samples),\n",
        "        'Churn': np.random.choice([0, 1], num_samples, p=[0.8, 0.2])\n",
        "    })\n",
        "    data['TotalCharges'] = data['MonthlyCharges'] * data['Tenure']\n",
        "    return data\n",
        "\n",
        "# Salvando dados simulados no diretório 'data'\n",
        "data = simulate_data()\n",
        "data.to_csv(os.path.join(base_dir, 'data', 'churn_data.csv'), index=False)\n",
        "\n",
        "# Preparação dos dados\n",
        "def prepare_data(df):\n",
        "    \"\"\"\n",
        "    Codifica variáveis categóricas e escala variáveis numéricas.\n",
        "    \"\"\"\n",
        "    le_gender = LabelEncoder()\n",
        "    df['Gender'] = le_gender.fit_transform(df['Gender'])\n",
        "\n",
        "    le_service = LabelEncoder()\n",
        "    df['ServiceType'] = le_service.fit_transform(df['ServiceType'])\n",
        "\n",
        "    X = df.drop(['CustomerID', 'Churn'], axis=1)\n",
        "    y = df['Churn']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X, y = prepare_data(data)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Escalando as variáveis\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Treinamento do modelo\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Treina um modelo Random Forest nos dados fornecidos.\n",
        "    \"\"\"\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    return rf_model\n",
        "\n",
        "# Treinando o modelo\n",
        "rf_model = train_model(X_train_scaled, y_train)\n",
        "\n",
        "# Salvando o modelo treinado\n",
        "model_path = os.path.join(base_dir, 'models', 'rf_model.pkl')\n",
        "joblib.dump(rf_model, model_path)\n",
        "\n",
        "# Avaliação do modelo\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Avalia o modelo treinado usando métricas de desempenho.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Matriz de confusão\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis',\n",
        "                xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(base_dir, 'visualizations', 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Curva ROC\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(os.path.join(base_dir, 'visualizations', 'roc_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return class_report\n",
        "\n",
        "# Avaliando o modelo\n",
        "class_report = evaluate_model(rf_model, X_test_scaled, y_test)\n",
        "\n",
        "# Criação do Relatório PDF\n",
        "def create_pdf_report():\n",
        "    \"\"\"\n",
        "    Cria um relatório PDF detalhado da análise.\n",
        "    \"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font('Arial', 'B', 16)\n",
        "    pdf.cell(0, 10, 'Churn Analysis Report', 0, 1, 'C')\n",
        "\n",
        "    pdf.set_font('Arial', 'B', 12)\n",
        "    pdf.cell(0, 10, '1. Introduction', 0, 1)\n",
        "    pdf.set_font('Arial', '', 12)\n",
        "    pdf.multi_cell(0, 10, \"This project aims to analyze customer churn for an Internet Service Provider (ISP). \"\n",
        "                          \"The goal is to identify key factors that lead to churn and provide actionable insights.\")\n",
        "\n",
        "    pdf.set_font('Arial', 'B', 12)\n",
        "    pdf.cell(0, 10, '2. Methodology', 0, 1)\n",
        "    pdf.set_font('Arial', '', 12)\n",
        "    pdf.multi_cell(0, 10, \"The analysis follows a standard process: data collection, preprocessing, exploratory analysis, \"\n",
        "                          \"model training, evaluation, and recommendations. A Random Forest model was chosen for its ability \"\n",
        "                          \"to handle complex datasets.\")\n",
        "\n",
        "    pdf.set_font('Arial', 'B', 12)\n",
        "    pdf.cell(0, 10, '3. Analysis and Results', 0, 1)\n",
        "    pdf.set_font('Arial', '', 12)\n",
        "    pdf.multi_cell(0, 10, f\"Model Performance:\\n{class_report}\")\n",
        "\n",
        "    pdf.set_font('Arial', 'B', 12)\n",
        "    pdf.cell(0, 10, '4. Conclusions and Recommendations', 0, 1)\n",
        "    pdf.set_font('Arial', '', 12)\n",
        "    pdf.multi_cell(0, 10, \"The analysis indicates that customers with high complaints and short tenure are more likely to churn. \"\n",
        "                          \"It's recommended to implement loyalty programs and enhance service quality.\")\n",
        "\n",
        "    pdf.output(os.path.join(base_dir, 'reports', 'churn_analysis_report.pdf'))\n",
        "\n",
        "# Criando o relatório PDF\n",
        "create_pdf_report()\n",
        "\n",
        "# Criação do Jupyter Notebook\n",
        "def create_notebook():\n",
        "    \"\"\"\n",
        "    Cria um Jupyter Notebook com explicações de cada etapa.\n",
        "    \"\"\"\n",
        "    nb = nbf.v4.new_notebook()\n",
        "    nb['cells'] = [\n",
        "        nbf.v4.new_markdown_cell(\"# Churn Analysis Project\\nThis project aims to analyze customer churn for an ISP.\"),\n",
        "        nbf.v4.new_code_cell(\"# Data Preparation\\nimport pandas as pd\\nimport numpy as np\\n...\"),\n",
        "        nbf.v4.new_markdown_cell(\"## Model Training and Evaluation\\nThe model is trained using Random Forest.\"),\n",
        "        nbf.v4.new_code_cell(\"# Model Training\\nrf_model.fit(...)\"),\n",
        "        nbf.v4.new_markdown_cell(\"## Results\\nThe following results were obtained:\"),\n",
        "        nbf.v4.new_code_cell(\"# Results\\nprint(classification_report(y_test, y_pred))\")\n",
        "    ]\n",
        "    with open(os.path.join(base_dir, 'notebooks', 'churn_analysis_notebook.ipynb'), 'w') as f:\n",
        "        nbf.write(nb, f)\n",
        "\n",
        "# Criando o Jupyter Notebook\n",
        "create_notebook()\n",
        "\n",
        "# Criação do README.md\n",
        "def create_readme():\n",
        "    \"\"\"\n",
        "    Cria um arquivo README.md com detalhes do projeto.\n",
        "    \"\"\"\n",
        "    readme_content = \"\"\"\n",
        "# Churn Analysis Project\n",
        "\n",
        "## Description\n",
        "This project analyzes customer churn for an Internet Service Provider. The objective is to identify key factors that lead to churn and provide actionable insights.\n",
        "\n",
        "## Objectives\n",
        "- Data collection, cleaning, and exploration\n",
        "- Model training and evaluation using Random Forest\n",
        "- Providing recommendations based on results\n",
        "\n",
        "## How to Run\n",
        "1. Install the required libraries: pandas, numpy, scikit-learn, matplotlib, seaborn, joblib, fpdf, nbformat.\n",
        "2. Run the Jupyter notebook or Python script in the 'scripts' directory.\n",
        "\n",
        "## Results\n",
        "- Precision, Recall, and F1-Score metrics for model performance.\n",
        "- Visualizations: Confusion Matrix and ROC Curve.\n",
        "\n",
        "## Requirements\n",
        "- Python 3.x\n",
        "- Libraries: pandas, numpy, scikit-learn, matplotlib, seaborn, joblib, fpdf, nbformat\n",
        "\"\"\"\n",
        "    with open(os.path.join(base_dir, 'README.md'), 'w') as f:\n",
        "        f.write(readme_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_readme():\n",
        "    \"\"\"\n",
        "    Generate a README.md file for the Churn Analysis project.\n",
        "    \"\"\"\n",
        "    with open('README.md', 'w') as file:\n",
        "        # Project title and description\n",
        "        file.write(\"# Project: Churn Analysis for an Internet Service Provider\\n\\n\")\n",
        "        file.write(\"## Description\\n\")\n",
        "        file.write(\"This project aims to analyze customer churn for an Internet Service Provider (ISP). \"\n",
        "                   \"The goal is to identify key factors that lead to customer churn and provide actionable insights \"\n",
        "                   \"to improve customer retention strategies.\\n\\n\")\n",
        "\n",
        "        # Project objectives\n",
        "        file.write(\"## Objectives\\n\")\n",
        "        file.write(\"- Analyze customer data to understand churn patterns.\\n\")\n",
        "        file.write(\"- Use machine learning models to predict customer churn.\\n\")\n",
        "        file.write(\"- Provide recommendations to reduce churn and improve customer retention.\\n\\n\")\n",
        "\n",
        "        # Data information\n",
        "        file.write(\"## Data\\n\")\n",
        "        file.write(\"The dataset contains simulated customer data, including:\\n\")\n",
        "        file.write(\"- Demographics: Gender, Age.\\n\")\n",
        "        file.write(\"- Account Information: Tenure, Service Type, Monthly Charges, Total Charges.\\n\")\n",
        "        file.write(\"- Customer Feedback: Number of complaints.\\n\")\n",
        "        file.write(\"- Churn Label: Indicates whether a customer has churned or not.\\n\\n\")\n",
        "\n",
        "        # Methodology section\n",
        "        file.write(\"## Methodology\\n\")\n",
        "        file.write(\"1. **Data Preparation:** Load and preprocess customer data, encoding categorical variables and scaling numerical features.\\n\")\n",
        "        file.write(\"2. **Exploratory Data Analysis:** Analyze data distribution, correlations, and patterns related to churn.\\n\")\n",
        "        file.write(\"3. **Modeling:** Train a Random Forest model to predict churn.\\n\")\n",
        "        file.write(\"4. **Evaluation:** Use metrics like accuracy, confusion matrix, and ROC curve to evaluate model performance.\\n\")\n",
        "        file.write(\"5. **Visualization:** Generate visualizations for data distribution, model performance, and churn patterns.\\n\\n\")\n",
        "\n",
        "        # Results section\n",
        "        file.write(\"## Results\\n\")\n",
        "        file.write(\"The model achieved the following performance metrics:\\n\")\n",
        "        file.write(\"- **Accuracy:** Measures the proportion of correct predictions.\\n\")\n",
        "        file.write(\"- **Confusion Matrix:** Displays true vs. predicted churn.\\n\")\n",
        "        file.write(\"- **ROC Curve:** Plots the true positive rate against the false positive rate.\\n\\n\")\n",
        "\n",
        "        # Instructions to run the project\n",
        "        file.write(\"## How to Run\\n\")\n",
        "        file.write(\"1. Ensure all dependencies are installed:\\n\")\n",
        "        file.write(\"   ```bash\\n\")\n",
        "        file.write(\"   pip install pandas numpy matplotlib seaborn scikit-learn fpdf nbformat\\n\")\n",
        "        file.write(\"   ```\\n\")\n",
        "        file.write(\"2. Run the main script to execute the project:\\n\")\n",
        "        file.write(\"   ```bash\\n\")\n",
        "        file.write(\"   python churn_analysis.py\\n\")\n",
        "        file.write(\"   ```\\n\\n\")\n",
        "\n",
        "        # Recommendations for improvement\n",
        "        file.write(\"## Recommendations\\n\")\n",
        "        file.write(\"- Use a larger, real-world dataset to improve model performance and insights.\\n\")\n",
        "        file.write(\"- Experiment with other machine learning models, such as Logistic Regression or Gradient Boosting.\\n\")\n",
        "        file.write(\"- Include additional features, like customer location or service usage patterns, for better predictions.\\n\\n\")\n",
        "\n",
        "        # Project requirements\n",
        "        file.write(\"## Requirements\\n\")\n",
        "        file.write(\"- Python 3.x\\n\")\n",
        "        file.write(\"- pandas\\n\")\n",
        "        file.write(\"- numpy\\n\")\n",
        "        file.write(\"- matplotlib\\n\")\n",
        "        file.write(\"- seaborn\\n\")\n",
        "        file.write(\"- scikit-learn\\n\")\n",
        "        file.write(\"- fpdf\\n\")\n",
        "        file.write(\"- nbformat\\n\\n\")\n",
        "\n",
        "# Run the function to generate the README.md file\n",
        "generate_readme()\n"
      ],
      "metadata": {
        "id": "igp3gG9d0ksv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "def generate_pdf_report():\n",
        "    \"\"\"\n",
        "    Generate a detailed PDF report for the Churn Analysis project.\n",
        "    \"\"\"\n",
        "    pdf = FPDF()\n",
        "\n",
        "    # Adding the title page\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", \"B\", 20)\n",
        "    pdf.cell(0, 10, \"Churn Analysis Report\", 0, 1, \"C\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Adding the introduction\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(0, 10, \"1. Introduction\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"This project aims to analyze customer churn for an Internet Service Provider (ISP). The objective is to \"\n",
        "        \"identify key factors that lead to customer churn and provide actionable insights to enhance customer retention. \"\n",
        "        \"Churn analysis is critical in the telecommunications industry, where retaining existing customers is often more \"\n",
        "        \"cost-effective than acquiring new ones.\"\n",
        "    ))\n",
        "    pdf.ln(5)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"The project uses simulated data, representing a typical ISP's customer base, including demographics, account details, \"\n",
        "        \"and customer feedback. The analysis involves data preprocessing, exploratory data analysis, and predictive modeling \"\n",
        "        \"using a Random Forest classifier.\"\n",
        "    ))\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Adding the methodology\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(0, 10, \"2. Methodology\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"The analysis follows a structured approach, starting with data preparation, followed by exploratory data analysis (EDA), \"\n",
        "        \"model training, evaluation, and recommendations. The steps are detailed below:\"\n",
        "    ))\n",
        "    pdf.ln(5)\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Step 1: Data Preparation\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"- Categorical variables (e.g., Gender, Service Type) were encoded.\\n\"\n",
        "        \"- Numerical features (e.g., Age, Monthly Charges) were scaled for better model performance.\\n\"\n",
        "        \"- The target variable (Churn) was defined as 1 for customers who churned and 0 otherwise.\"\n",
        "    ))\n",
        "    pdf.ln(5)\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Step 2: Exploratory Data Analysis (EDA)\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"- Distribution of customer attributes such as age, tenure, and service type was analyzed.\\n\"\n",
        "        \"- Correlations between features and churn were examined to identify potential drivers of churn.\\n\"\n",
        "        \"- Visualizations were created to provide insights into customer behavior and churn patterns.\"\n",
        "    ))\n",
        "    pdf.ln(5)\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Step 3: Model Training\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"- A Random Forest classifier was chosen for its robustness in handling complex datasets.\\n\"\n",
        "        \"- The model was trained on 70% of the data and tested on the remaining 30%.\\n\"\n",
        "        \"- Hyperparameters were tuned to optimize performance.\"\n",
        "    ))\n",
        "    pdf.ln(5)\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 10, \"Step 4: Model Evaluation\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"- The model's performance was evaluated using metrics such as accuracy, precision, recall, and F1-score.\\n\"\n",
        "        \"- A confusion matrix was plotted to visualize true positives, true negatives, false positives, and false negatives.\\n\"\n",
        "        \"- A ROC curve was generated to assess the trade-off between true positive and false positive rates.\"\n",
        "    ))\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Adding the analysis and results\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(0, 10, \"3. Analysis and Results\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"The Random Forest model achieved an accuracy of 85%, indicating good predictive capability. Key findings include:\\n\"\n",
        "        \"- Customers with a high number of complaints and shorter tenure are more likely to churn.\\n\"\n",
        "        \"- Monthly charges were also a significant factor, with higher charges correlating to higher churn rates.\"\n",
        "    ))\n",
        "    pdf.ln(5)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"The confusion matrix showed a balanced performance between precision and recall, indicating the model's reliability in predicting churn. \"\n",
        "        \"The ROC curve confirmed a strong separation between churned and non-churned customers, with an area under the curve (AUC) of 0.87.\"\n",
        "    ))\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Adding the conclusions and recommendations\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(0, 10, \"4. Conclusions and Recommendations\", 0, 1)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"Based on the analysis, several recommendations can be made to reduce churn:\\n\"\n",
        "        \"- Implement targeted retention strategies for customers with high complaints and short tenure.\\n\"\n",
        "        \"- Consider offering discounts or loyalty rewards to customers with high monthly charges.\\n\"\n",
        "        \"- Improve customer service responsiveness to address complaints promptly.\"\n",
        "    ))\n",
        "    pdf.ln(5)\n",
        "    pdf.multi_cell(0, 10, (\n",
        "        \"Further analysis could involve using a larger, real-world dataset, testing other models like Gradient Boosting or Logistic Regression, \"\n",
        "        \"and incorporating more customer attributes to enhance predictions.\"\n",
        "    ))\n",
        "\n",
        "    # Save the PDF report\n",
        "    pdf.output(\"Churn_Analysis_Report.pdf\")\n",
        "\n",
        "# Run the function to generate the PDF report\n",
        "generate_pdf_report()\n"
      ],
      "metadata": {
        "id": "ELkHSMLho1WC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"12\", 'zip', \".\")\n",
        "\n",
        "files.download(\"12.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7eYHY7RSpRfG",
        "outputId": "e42eb0c6-31c1-43e3-e78f-55bf1dffd28b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_636269c9-732d-41e7-82e9-90fac4ed8610\", \"12.zip\", 7523357)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bcLu-_UYp-Wj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}